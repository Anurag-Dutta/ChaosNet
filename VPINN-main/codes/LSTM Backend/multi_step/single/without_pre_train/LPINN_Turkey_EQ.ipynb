{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xSItPJipBaZ5"
      },
      "source": [
        "## Gathering Dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Importing Required Libraries_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6LN-zXiLcM",
        "outputId": "1a821417-b2e6-4bf3-ad0b-494bb85bea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
            "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.21.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7.3->pandas->hampel) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install hampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "By_d9uXpaFvZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from hampel import hampel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from numpy import array"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_TURKEY EARTHQUAKE_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       4.4\n",
            "1       5.2\n",
            "2       4.8\n",
            "3       4.4\n",
            "4       5.7\n",
            "       ... \n",
            "6569    4.5\n",
            "6570    4.0\n",
            "6571    4.4\n",
            "6572    4.1\n",
            "6573    4.0\n",
            "Name: Magnitude, Length: 6574, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"datasets/eq.csv\")\n",
        "training_set = data.iloc[:, 4]\n",
        "print(training_set)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Gradients"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Calculating the value of_ $\\frac{dx}{dt}$, _and_ $\\frac{d^2x}{dt^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.9\n",
            "1       0.8\n",
            "2      -0.4\n",
            "3      -0.4\n",
            "4       1.3\n",
            "5      -1.4\n",
            "       ... \n",
            "6569    0.4\n",
            "6570   -0.5\n",
            "6571    0.4\n",
            "6572   -0.3\n",
            "6573   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "2      -1.200000e+00\n",
            "3       8.881784e-16\n",
            "4       1.700000e+00\n",
            "5      -2.700000e+00\n",
            "6       1.100000e+00\n",
            "            ...     \n",
            "6569    1.500000e+00\n",
            "6570   -9.000000e-01\n",
            "6571    9.000000e-01\n",
            "6572   -7.000000e-01\n",
            "6573    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "t_diff = 1 # Daily Data\n",
        "print(training_set.max())\n",
        "gradient_t = (training_set.diff()/t_diff).iloc[1:]\n",
        "print(gradient_t)\n",
        "gradient_tt = (gradient_t.diff()/t_diff).iloc[1:]\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       0.8\n",
            "1      -0.4\n",
            "2      -0.4\n",
            "3       1.3\n",
            "4      -1.4\n",
            "       ... \n",
            "6568    0.4\n",
            "6569   -0.5\n",
            "6570    0.4\n",
            "6571   -0.3\n",
            "6572   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "0      -1.200000e+00\n",
            "1       8.881784e-16\n",
            "2       1.700000e+00\n",
            "3      -2.700000e+00\n",
            "4       1.100000e+00\n",
            "            ...     \n",
            "6567    1.500000e+00\n",
            "6568   -9.000000e-01\n",
            "6569    9.000000e-01\n",
            "6570   -7.000000e-01\n",
            "6571    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "training_set = training_set.reset_index(drop=True)\n",
        "gradient_t = gradient_t.reset_index(drop=True)\n",
        "gradient_tt = gradient_tt.reset_index(drop=True)\n",
        "print(gradient_t)\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6573,)\n",
            "()\n"
          ]
        }
      ],
      "source": [
        "print(gradient_t.shape)\n",
        "print(training_set.shape[:-1])\n",
        "df = pd.concat((training_set[:-1], gradient_t), axis=1)\n",
        "gradient_tt.columns = [\"grad_tt\"]\n",
        "df = pd.concat((df[:-1], gradient_tt), axis=1)\n",
        "df.columns = ['y_t', 'grad_t', 'grad_tt']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-5esyHu5aFvg"
      },
      "source": [
        "## Plot of the External Forcing from Chaotic Differential Equation (_Lorrenz Equation_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "ym4xWUUxaFvg",
        "outputId": "45058d71-6952-4a40-afa5-84c2f42197b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "s=10\n",
        "r=28\n",
        "L = df.iloc[:, 2] + df.iloc[:, 1]*(1 + s) - df.iloc[:, 0] * s * (r - 1)\n",
        "L.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEywnwaFvh"
      },
      "source": [
        "## Preprocessing the data into supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6V9dXqzdaFvh"
      },
      "outputs": [],
      "source": [
        "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "      if i == 0:\n",
        "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "      else:\n",
        "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "       agg.dropna(inplace=True)\n",
        "    return agg    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI8Yfkw6oA0l",
        "outputId": "01d7b72a-3934-4f62-ca10-27be3fc0310c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['var1(t-10)', 'var2(t-10)', 'var3(t-10)', 'var1(t-9)', 'var2(t-9)',\n",
              "       'var3(t-9)', 'var1(t-8)', 'var2(t-8)', 'var3(t-8)', 'var1(t-7)',\n",
              "       ...\n",
              "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
              "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
              "       'var2(t+364)', 'var3(t+364)'],\n",
              "      dtype='object', length=1125)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "dat.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrzSrT1HnyfH",
        "outputId": "f37830fc-a93f-4216-e5d6-8e44bb1f83ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    var1(t-10)  var1(t-9)  var1(t-8)  var1(t-7)  var1(t-6)  var1(t-5)  \\\n",
            "10         4.4        5.2        4.8        4.4        5.7        4.3   \n",
            "11         5.2        4.8        4.4        5.7        4.3        4.0   \n",
            "12         4.8        4.4        5.7        4.3        4.0        4.8   \n",
            "13         4.4        5.7        4.3        4.0        4.8        5.2   \n",
            "14         5.7        4.3        4.0        4.8        5.2        5.1   \n",
            "\n",
            "    var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)  ...  var3(t+361)  var1(t+362)  \\\n",
            "10        4.0        4.8        5.2        5.1  ...         -1.3          5.7   \n",
            "11        4.8        5.2        5.1        4.7  ...          1.6          4.9   \n",
            "12        5.2        5.1        4.7        4.6  ...         -1.1          5.7   \n",
            "13        5.1        4.7        4.6        4.4  ...          0.1          5.4   \n",
            "14        4.7        4.6        4.4        4.7  ...          0.1          5.2   \n",
            "\n",
            "    var2(t+362)  var3(t+362)  var1(t+363)  var2(t+363)  var3(t+363)  \\\n",
            "10         -0.8          1.6          4.9          0.8         -1.1   \n",
            "11          0.8         -1.1          5.7         -0.3          0.1   \n",
            "12         -0.3          0.1          5.4         -0.2          0.1   \n",
            "13         -0.2          0.1          5.2         -0.1          0.2   \n",
            "14         -0.1          0.2          5.1          0.1          0.1   \n",
            "\n",
            "    var1(t+364)  var2(t+364)  var3(t+364)  \n",
            "10          5.7         -0.3          0.1  \n",
            "11          5.4         -0.2          0.1  \n",
            "12          5.2         -0.1          0.2  \n",
            "13          5.1          0.1          0.1  \n",
            "14          5.2          0.2         -0.1  \n",
            "\n",
            "[5 rows x 1105 columns]\n",
            "Index(['var1(t-10)', 'var1(t-9)', 'var1(t-8)', 'var1(t-7)', 'var1(t-6)',\n",
            "       'var1(t-5)', 'var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)',\n",
            "       ...\n",
            "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
            "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
            "       'var2(t+364)', 'var3(t+364)'],\n",
            "      dtype='object', length=1105)\n"
          ]
        }
      ],
      "source": [
        "data = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "data.drop(['var2(t-10)', 'var3(t-10)', 'var2(t-9)', 'var3(t-9)', 'var2(t-8)',\n",
        "       'var3(t-8)', 'var2(t-7)', 'var3(t-7)', 'var2(t-6)', 'var3(t-6)',\n",
        "       'var2(t-5)', 'var3(t-5)', 'var2(t-4)', 'var3(t-4)', 'var2(t-2)',\n",
        "       'var3(t-2)', 'var2(t-1)', 'var3(t-1)','var2(t-3)', 'var3(t-3)'], axis = 1, inplace = True)#,18,19\n",
        "print(data.head())\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKb5l_gUaFvi"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVOndQpQaFvi",
        "outputId": "56d94a79-1d05-4ff7-d70e-e2d8d00c83c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4958, 1, 1096) (4958, 9) (1240, 1, 1096) (1240, 9)\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(data) * 0.8)\n",
        "test_size = len(data) - train_size\n",
        "train_1 = np.array(data[0:train_size])\n",
        "test_1 = np.array(data[train_size:len(data)])\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train = scaler.fit_transform(train_1)\n",
        "test = scaler.transform(test_1)\n",
        "trainY = train[:,-9:]\n",
        "trainX = train[:,:-9]\n",
        "testY = test[:,-9:]\n",
        "testX = test[:,:-9]\n",
        "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = testX.reshape((testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-D_j8UaFvj"
      },
      "source": [
        "## Defining the Physical Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Jw7vitLaFvj"
      },
      "outputs": [],
      "source": [
        "s = tf.Variable(10, name=\"sigma\", trainable=True, dtype=tf.float32)\n",
        "r = tf.Variable(28, name=\"rhow\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def phys(y_pred, y_true):\n",
        "    return mean_absolute_error((y_true[:, 2] + y_true[:, 1]*(1 + s) - y_true[:, 0] * s * (r - 1)), (y_pred[:, 2] + y_pred[:, 1]*(1 + s) - y_pred[:, 0] * s * (r - 1)))\n",
        "\n",
        "def phys2(y_pred, y_real):\n",
        "    pred = y_pred[2:]-2*y_pred[1:-1]-y_pred[:-2] + (y_pred[1:-1]-y_pred[:-2])*(1 + s) - y_pred[:-2] * s * (r - 1)\n",
        "    real = y_real[2:]-2*y_real[1:-1]-y_real[:-2] + (y_real[1:-1]-y_real[:-2])*(1 + s) - y_real[:-2] * s * (r - 1)\n",
        "    return(mean_absolute_error(pred, real))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--1LVbHOBSIy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "874xZ-_u7X_s",
        "outputId": "883c11bb-1fe0-48af-e119-65529c186443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "78/78 [==============================] - 4s 15ms/step - loss: 202.2493 - val_loss: 0.0505\n",
            "Epoch 2/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.1239 - val_loss: 0.0490\n",
            "Epoch 3/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.1085 - val_loss: 0.0474\n",
            "Epoch 4/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0970 - val_loss: 0.0450\n",
            "Epoch 5/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0871 - val_loss: 0.0429\n",
            "Epoch 6/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0790 - val_loss: 0.0410\n",
            "Epoch 7/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0393\n",
            "Epoch 8/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0669 - val_loss: 0.0378\n",
            "Epoch 9/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0625 - val_loss: 0.0365\n",
            "Epoch 10/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0353\n",
            "Epoch 11/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0558 - val_loss: 0.0342\n",
            "Epoch 12/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.0332\n",
            "Epoch 13/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0513 - val_loss: 0.0323\n",
            "Epoch 14/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0496 - val_loss: 0.0315\n",
            "Epoch 15/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0481 - val_loss: 0.0307\n",
            "Epoch 16/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0469 - val_loss: 0.0300\n",
            "Epoch 17/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0459 - val_loss: 0.0294\n",
            "Epoch 18/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0450 - val_loss: 0.0288\n",
            "Epoch 19/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0443 - val_loss: 0.0282\n",
            "Epoch 20/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0436 - val_loss: 0.0277\n",
            "Epoch 21/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0430 - val_loss: 0.0272\n",
            "Epoch 22/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0425 - val_loss: 0.0267\n",
            "Epoch 23/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0420 - val_loss: 0.0263\n",
            "Epoch 24/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0416 - val_loss: 0.0258\n",
            "Epoch 25/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0412 - val_loss: 0.0254\n",
            "Epoch 26/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0408 - val_loss: 0.0251\n",
            "Epoch 27/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0404 - val_loss: 0.0247\n",
            "Epoch 28/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0401 - val_loss: 0.0243\n",
            "Epoch 29/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0398 - val_loss: 0.0240\n",
            "Epoch 30/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0394 - val_loss: 0.0237\n",
            "Epoch 31/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0391 - val_loss: 0.0234\n",
            "Epoch 32/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0388 - val_loss: 0.0231\n",
            "Epoch 33/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0386 - val_loss: 0.0228\n",
            "Epoch 34/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0383 - val_loss: 0.0225\n",
            "Epoch 35/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0380 - val_loss: 0.0222\n",
            "Epoch 36/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0377 - val_loss: 0.0220\n",
            "Epoch 37/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0375 - val_loss: 0.0217\n",
            "Epoch 38/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0373 - val_loss: 0.0215\n",
            "Epoch 39/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0370 - val_loss: 0.0212\n",
            "Epoch 40/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0368 - val_loss: 0.0210\n",
            "Epoch 41/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0366 - val_loss: 0.0208\n",
            "Epoch 42/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0206\n",
            "Epoch 43/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0361 - val_loss: 0.0204\n",
            "Epoch 44/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0359 - val_loss: 0.0202\n",
            "Epoch 45/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0357 - val_loss: 0.0200\n",
            "Epoch 46/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0356 - val_loss: 0.0198\n",
            "Epoch 47/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0354 - val_loss: 0.0196\n",
            "Epoch 48/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0352 - val_loss: 0.0194\n",
            "Epoch 49/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0350 - val_loss: 0.0192\n",
            "Epoch 50/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0349 - val_loss: 0.0191\n",
            "Epoch 51/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0347 - val_loss: 0.0189\n",
            "Epoch 52/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0188\n",
            "Epoch 53/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0344 - val_loss: 0.0186\n",
            "Epoch 54/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0185\n",
            "Epoch 55/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0183\n",
            "Epoch 56/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0340 - val_loss: 0.0182\n",
            "Epoch 57/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0338 - val_loss: 0.0181\n",
            "Epoch 58/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0337 - val_loss: 0.0179\n",
            "Epoch 59/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0336 - val_loss: 0.0178\n",
            "Epoch 60/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0335 - val_loss: 0.0177\n",
            "Epoch 61/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0334 - val_loss: 0.0176\n",
            "Epoch 62/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0333 - val_loss: 0.0175\n",
            "Epoch 63/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0332 - val_loss: 0.0173\n",
            "Epoch 64/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0331 - val_loss: 0.0172\n",
            "Epoch 65/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0330 - val_loss: 0.0171\n",
            "Epoch 66/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0329 - val_loss: 0.0170\n",
            "Epoch 67/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0328 - val_loss: 0.0169\n",
            "Epoch 68/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0327 - val_loss: 0.0168\n",
            "Epoch 69/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0326 - val_loss: 0.0167\n",
            "Epoch 70/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0325 - val_loss: 0.0166\n",
            "Epoch 71/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0324 - val_loss: 0.0165\n",
            "Epoch 72/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.0164\n",
            "Epoch 73/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0323 - val_loss: 0.0164\n",
            "Epoch 74/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0322 - val_loss: 0.0163\n",
            "Epoch 75/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.0162\n",
            "Epoch 76/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0321 - val_loss: 0.0161\n",
            "Epoch 77/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0320 - val_loss: 0.0161\n",
            "Epoch 78/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0319 - val_loss: 0.0160\n",
            "Epoch 79/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0319 - val_loss: 0.0159\n",
            "Epoch 80/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0318 - val_loss: 0.0159\n",
            "Epoch 81/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0318 - val_loss: 0.0158\n",
            "Epoch 82/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0317 - val_loss: 0.0157\n",
            "Epoch 83/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0316 - val_loss: 0.0157\n",
            "Epoch 84/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0316 - val_loss: 0.0156\n",
            "Epoch 85/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0315 - val_loss: 0.0156\n",
            "Epoch 86/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0315 - val_loss: 0.0155\n",
            "Epoch 87/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0155\n",
            "Epoch 88/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0314 - val_loss: 0.0154\n",
            "Epoch 89/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0313 - val_loss: 0.0154\n",
            "Epoch 90/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0313 - val_loss: 0.0153\n",
            "Epoch 91/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0312 - val_loss: 0.0153\n",
            "Epoch 92/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0312 - val_loss: 0.0152\n",
            "Epoch 93/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0311 - val_loss: 0.0152\n",
            "Epoch 94/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0311 - val_loss: 0.0151\n",
            "Epoch 95/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0310 - val_loss: 0.0151\n",
            "Epoch 96/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.0150\n",
            "Epoch 97/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0309 - val_loss: 0.0150\n",
            "Epoch 98/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0308 - val_loss: 0.0149\n",
            "Epoch 99/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0307 - val_loss: 0.0148\n",
            "Epoch 100/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0306 - val_loss: 0.0147\n",
            "Epoch 101/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0303 - val_loss: 0.0145\n",
            "Epoch 102/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0300 - val_loss: 0.0142\n",
            "Epoch 103/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0294 - val_loss: 0.0139\n",
            "Epoch 104/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0290 - val_loss: 0.0136\n",
            "Epoch 105/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0287 - val_loss: 0.0135\n",
            "Epoch 106/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0285 - val_loss: 0.0134\n",
            "Epoch 107/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0283 - val_loss: 0.0133\n",
            "Epoch 108/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0282 - val_loss: 0.0132\n",
            "Epoch 109/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0281 - val_loss: 0.0132\n",
            "Epoch 110/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0280 - val_loss: 0.0131\n",
            "Epoch 111/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0279 - val_loss: 0.0130\n",
            "Epoch 112/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0278 - val_loss: 0.0129\n",
            "Epoch 113/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0277 - val_loss: 0.0129\n",
            "Epoch 114/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0276 - val_loss: 0.0128\n",
            "Epoch 115/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0276 - val_loss: 0.0128\n",
            "Epoch 116/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0275 - val_loss: 0.0127\n",
            "Epoch 117/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0274 - val_loss: 0.0126\n",
            "Epoch 118/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0273 - val_loss: 0.0126\n",
            "Epoch 119/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0125\n",
            "Epoch 120/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0271 - val_loss: 0.0125\n",
            "Epoch 121/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0270 - val_loss: 0.0124\n",
            "Epoch 122/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0123\n",
            "Epoch 123/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0269 - val_loss: 0.0123\n",
            "Epoch 124/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0268 - val_loss: 0.0122\n",
            "Epoch 125/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0267 - val_loss: 0.0122\n",
            "Epoch 126/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0121\n",
            "Epoch 127/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0265 - val_loss: 0.0121\n",
            "Epoch 128/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0264 - val_loss: 0.0121\n",
            "Epoch 129/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0120\n",
            "Epoch 130/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0262 - val_loss: 0.0119\n",
            "Epoch 131/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0261 - val_loss: 0.0119\n",
            "Epoch 132/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0260 - val_loss: 0.0118\n",
            "Epoch 133/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0259 - val_loss: 0.0117\n",
            "Epoch 134/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0258 - val_loss: 0.0116\n",
            "Epoch 135/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0257 - val_loss: 0.0116\n",
            "Epoch 136/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0256 - val_loss: 0.0115\n",
            "Epoch 137/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0255 - val_loss: 0.0115\n",
            "Epoch 138/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0114\n",
            "Epoch 139/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0254 - val_loss: 0.0113\n",
            "Epoch 140/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0113\n",
            "Epoch 141/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0252 - val_loss: 0.0112\n",
            "Epoch 142/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0251 - val_loss: 0.0112\n",
            "Epoch 143/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0250 - val_loss: 0.0111\n",
            "Epoch 144/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0249 - val_loss: 0.0110\n",
            "Epoch 145/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0248 - val_loss: 0.0110\n",
            "Epoch 146/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0247 - val_loss: 0.0109\n",
            "Epoch 147/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0246 - val_loss: 0.0108\n",
            "Epoch 148/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0243 - val_loss: 0.0107\n",
            "Epoch 149/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0240 - val_loss: 0.0104\n",
            "Epoch 150/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0238 - val_loss: 0.0102\n",
            "Epoch 151/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0234 - val_loss: 0.0101\n",
            "Epoch 152/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0232 - val_loss: 0.0101\n",
            "Epoch 153/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0100\n",
            "Epoch 154/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0100\n",
            "Epoch 155/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0231 - val_loss: 0.0100\n",
            "Epoch 156/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0230 - val_loss: 0.0100\n",
            "Epoch 157/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0229 - val_loss: 0.0100\n",
            "Epoch 158/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0228 - val_loss: 0.0099\n",
            "Epoch 159/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0228 - val_loss: 0.0099\n",
            "Epoch 160/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.0099\n",
            "Epoch 161/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.0099\n",
            "Epoch 162/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0226 - val_loss: 0.0097\n",
            "Epoch 163/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0097\n",
            "Epoch 164/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0224 - val_loss: 0.0097\n",
            "Epoch 165/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0224 - val_loss: 0.0096\n",
            "Epoch 166/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0223 - val_loss: 0.0096\n",
            "Epoch 167/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0222 - val_loss: 0.0096\n",
            "Epoch 168/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0222 - val_loss: 0.0095\n",
            "Epoch 169/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0221 - val_loss: 0.0095\n",
            "Epoch 170/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0220 - val_loss: 0.0095\n",
            "Epoch 171/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0220 - val_loss: 0.0095\n",
            "Epoch 172/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.0095\n",
            "Epoch 173/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0218 - val_loss: 0.0094\n",
            "Epoch 174/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0217 - val_loss: 0.0094\n",
            "Epoch 175/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0094\n",
            "Epoch 176/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0094\n",
            "Epoch 177/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0215 - val_loss: 0.0093\n",
            "Epoch 178/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0215 - val_loss: 0.0093\n",
            "Epoch 179/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0214 - val_loss: 0.0093\n",
            "Epoch 180/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0214 - val_loss: 0.0093\n",
            "Epoch 181/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0214 - val_loss: 0.0093\n",
            "Epoch 182/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0092\n",
            "Epoch 183/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0213 - val_loss: 0.0092\n",
            "Epoch 184/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0213 - val_loss: 0.0094\n",
            "Epoch 185/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0212 - val_loss: 0.0092\n",
            "Epoch 186/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0219 - val_loss: 0.0092\n",
            "Epoch 187/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.0091\n",
            "Epoch 188/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0088\n",
            "Epoch 189/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0087\n",
            "Epoch 190/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0198 - val_loss: 0.0087\n",
            "Epoch 191/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0198 - val_loss: 0.0087\n",
            "Epoch 192/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0199 - val_loss: 0.0087\n",
            "Epoch 193/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0204 - val_loss: 0.0091\n",
            "Epoch 194/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0206 - val_loss: 0.0099\n",
            "Epoch 195/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0197 - val_loss: 0.0096\n",
            "Epoch 196/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0193 - val_loss: 0.0085\n",
            "Epoch 197/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0200 - val_loss: 0.0091\n",
            "Epoch 198/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0188 - val_loss: 0.0084\n",
            "Epoch 199/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0202 - val_loss: 0.0087\n",
            "Epoch 200/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.0084\n",
            "Epoch 201/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0188 - val_loss: 0.0084\n",
            "Epoch 202/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0188 - val_loss: 0.0084\n",
            "Epoch 203/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.0086\n",
            "Epoch 204/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0189 - val_loss: 0.0084\n",
            "Epoch 205/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0190 - val_loss: 0.0085\n",
            "Epoch 206/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0190 - val_loss: 0.0086\n",
            "Epoch 207/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0188 - val_loss: 0.0085\n",
            "Epoch 208/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0187 - val_loss: 0.0085\n",
            "Epoch 209/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0084\n",
            "Epoch 210/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0185 - val_loss: 0.0084\n",
            "Epoch 211/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0184 - val_loss: 0.0084\n",
            "Epoch 212/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0183 - val_loss: 0.0084\n",
            "Epoch 213/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0183 - val_loss: 0.0084\n",
            "Epoch 214/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.0083\n",
            "Epoch 215/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0083\n",
            "Epoch 216/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.0083\n",
            "Epoch 217/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.0083\n",
            "Epoch 218/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0182 - val_loss: 0.0083\n",
            "Epoch 219/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 220/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 221/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 222/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 223/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 224/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 225/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0181 - val_loss: 0.0083\n",
            "Epoch 226/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.0084\n",
            "Epoch 227/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.0084\n",
            "Epoch 228/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.0084\n",
            "Epoch 229/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0179 - val_loss: 0.0083\n",
            "Epoch 230/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0178 - val_loss: 0.0083\n",
            "Epoch 231/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.0083\n",
            "Epoch 232/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0177 - val_loss: 0.0083\n",
            "Epoch 233/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0177 - val_loss: 0.0083\n",
            "Epoch 234/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0176 - val_loss: 0.0083\n",
            "Epoch 235/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0176 - val_loss: 0.0083\n",
            "Epoch 236/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0175 - val_loss: 0.0083\n",
            "Epoch 237/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 238/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 239/500\n",
            "78/78 [==============================] - 1s 13ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 240/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 241/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 242/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 243/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0176 - val_loss: 0.0086\n",
            "Epoch 244/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 245/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0175 - val_loss: 0.0083\n",
            "Epoch 246/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 247/500\n",
            "78/78 [==============================] - 3s 44ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 248/500\n",
            "78/78 [==============================] - 1s 16ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 249/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 250/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 251/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 252/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 253/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 254/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 255/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 256/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0083\n",
            "Epoch 257/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0180 - val_loss: 0.0083\n",
            "Epoch 258/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0083\n",
            "Epoch 259/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0084\n",
            "Epoch 260/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 261/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 262/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 263/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 264/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 265/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 266/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 267/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 268/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 269/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 270/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 271/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 272/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 273/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 274/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 275/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 276/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 277/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 278/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 279/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 280/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 281/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 282/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 283/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 284/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0084\n",
            "Epoch 285/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 286/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 287/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 288/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 289/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 290/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 291/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 292/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 293/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 294/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 295/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 296/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 297/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 298/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0165 - val_loss: 0.0085\n",
            "Epoch 299/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0086\n",
            "Epoch 300/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0086\n",
            "Epoch 301/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0165 - val_loss: 0.0086\n",
            "Epoch 302/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0165 - val_loss: 0.0087\n",
            "Epoch 303/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0164 - val_loss: 0.0086\n",
            "Epoch 304/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 305/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 306/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 307/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 308/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 309/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 310/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 311/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 312/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0087\n",
            "Epoch 313/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0246 - val_loss: 0.0087\n",
            "Epoch 314/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0086\n",
            "Epoch 315/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0086\n",
            "Epoch 316/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0086\n",
            "Epoch 317/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0161 - val_loss: 0.0087\n",
            "Epoch 318/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0087\n",
            "Epoch 319/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0087\n",
            "Epoch 320/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 321/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 322/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 323/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 324/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 325/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 326/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 327/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 328/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 329/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 330/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 331/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0164 - val_loss: 0.0086\n",
            "Epoch 332/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 333/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0091\n",
            "Epoch 334/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 335/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 336/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 337/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 338/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 339/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 340/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0086\n",
            "Epoch 341/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 342/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 343/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0085\n",
            "Epoch 344/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0085\n",
            "Epoch 345/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 346/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 347/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 348/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 349/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0084\n",
            "Epoch 350/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0084\n",
            "Epoch 351/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0085\n",
            "Epoch 352/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0084\n",
            "Epoch 353/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0098\n",
            "Epoch 354/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0084\n",
            "Epoch 355/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0301 - val_loss: 0.0092\n",
            "Epoch 356/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0084\n",
            "Epoch 357/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0084\n",
            "Epoch 358/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0084\n",
            "Epoch 359/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 360/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 361/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 362/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 363/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 364/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 365/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 366/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 367/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 368/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 369/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 370/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 371/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 372/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0085\n",
            "Epoch 373/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 374/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 375/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 376/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 377/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 378/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 379/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 380/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 381/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 382/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0085\n",
            "Epoch 383/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0085\n",
            "Epoch 384/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0085\n",
            "Epoch 385/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0164 - val_loss: 0.0086\n",
            "Epoch 386/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0162 - val_loss: 0.0086\n",
            "Epoch 387/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 388/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0088\n",
            "Epoch 389/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0162 - val_loss: 0.0086\n",
            "Epoch 390/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0086\n",
            "Epoch 391/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 392/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0086\n",
            "Epoch 393/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0087\n",
            "Epoch 394/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0087\n",
            "Epoch 395/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0086\n",
            "Epoch 396/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0088\n",
            "Epoch 397/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0092\n",
            "Epoch 398/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0086\n",
            "Epoch 399/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0087\n",
            "Epoch 400/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0088\n",
            "Epoch 401/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0161 - val_loss: 0.0089\n",
            "Epoch 402/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0083\n",
            "Epoch 403/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0087\n",
            "Epoch 404/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 405/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 406/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 407/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0089\n",
            "Epoch 408/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0162 - val_loss: 0.0087\n",
            "Epoch 409/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0160 - val_loss: 0.0087\n",
            "Epoch 410/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0161 - val_loss: 0.0090\n",
            "Epoch 411/500\n",
            "78/78 [==============================] - 1s 15ms/step - loss: 0.0160 - val_loss: 0.0088\n",
            "Epoch 412/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0162 - val_loss: 0.0091\n",
            "Epoch 413/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0160 - val_loss: 0.0088\n",
            "Epoch 414/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0089\n",
            "Epoch 415/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0161 - val_loss: 0.0089\n",
            "Epoch 416/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0088\n",
            "Epoch 417/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0091\n",
            "Epoch 418/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0082\n",
            "Epoch 419/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0106\n",
            "Epoch 420/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0184 - val_loss: 0.0082\n",
            "Epoch 421/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 422/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0108\n",
            "Epoch 423/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0159 - val_loss: 0.0088\n",
            "Epoch 424/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0159 - val_loss: 0.0099\n",
            "Epoch 425/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0085\n",
            "Epoch 426/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0164 - val_loss: 0.0084\n",
            "Epoch 427/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0210 - val_loss: 0.0085\n",
            "Epoch 428/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0083\n",
            "Epoch 429/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0083\n",
            "Epoch 430/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 431/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0158 - val_loss: 0.0083\n",
            "Epoch 432/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0083\n",
            "Epoch 433/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 434/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 435/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 436/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 437/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 438/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 439/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 440/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 441/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 442/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 443/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 444/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 445/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 446/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 447/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 448/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 449/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 450/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 451/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 452/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 453/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 454/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 455/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 456/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 457/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 458/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 459/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0155 - val_loss: 0.0083\n",
            "Epoch 460/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 461/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 462/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 463/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 464/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 465/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 466/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 467/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 468/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 469/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 470/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 471/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 472/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 473/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 474/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 475/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 476/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 477/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0152 - val_loss: 0.0083\n",
            "Epoch 478/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 479/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 480/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 481/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 482/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 483/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 484/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 485/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 486/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 487/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 488/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 489/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 490/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 491/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0149 - val_loss: 0.0083\n",
            "Epoch 492/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 493/500\n",
            "78/78 [==============================] - 1s 12ms/step - loss: 0.0149 - val_loss: 0.0083\n",
            "Epoch 494/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 495/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0148 - val_loss: 0.0083\n",
            "Epoch 496/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0149 - val_loss: 0.0083\n",
            "Epoch 497/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0148 - val_loss: 0.0083\n",
            "Epoch 498/500\n",
            "78/78 [==============================] - 1s 10ms/step - loss: 0.0150 - val_loss: 0.0083\n",
            "Epoch 499/500\n",
            "78/78 [==============================] - 1s 11ms/step - loss: 0.0147 - val_loss: 0.0083\n",
            "Epoch 500/500\n",
            "78/78 [==============================] - 1s 14ms/step - loss: 0.0148 - val_loss: 0.0083\n"
          ]
        }
      ],
      "source": [
        "s = tf.Variable(10, name=\"sigma\", trainable=True, dtype=tf.float32)\n",
        "r = tf.Variable(28, name=\"rhow\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
        "    squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
        "    squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
        "    squared_difference3 = tf.square((y_pred[:, 2] + y_pred[:, 1]*(1 + s) - y_pred[:, 0] * s * (r - 1)))\n",
        "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(Dense(9))\n",
        "model.compile(loss=loss_fn, optimizer='adam')\n",
        "history = model.fit(trainX, trainY, epochs=500, batch_size=64, validation_data=(testX, testY), shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 3s 5ms/step\n",
            "(1240, 9)\n",
            "(1240, 1096)\n",
            "Test RMSE: 0.471\n",
            "Test MAE: 0.313\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "yhat = model.predict(testX)\n",
        "print(yhat.shape)\n",
        "testX = testX.reshape((testX.shape[0], testX.shape[2]))\n",
        "print(testX.shape)\n",
        "inv_yhat = np.concatenate((testX, yhat), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat1 = inv_yhat[:, -3:]\n",
        "inv_yhat = inv_yhat[:, -3]\n",
        "inv_y = np.concatenate((testX, testY), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y1 = inv_y[:, -3:]\n",
        "inv_y = inv_y[:, -3]\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "mae = mean_absolute_error(inv_y, inv_yhat)\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
