{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xSItPJipBaZ5"
      },
      "source": [
        "## Gathering Dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Importing Required Libraries_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6LN-zXiLcM",
        "outputId": "1a821417-b2e6-4bf3-ad0b-494bb85bea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
            "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.21.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.3.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7.3->pandas->hampel) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install hampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "By_d9uXpaFvZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from hampel import hampel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from numpy import array"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_TURKEY EARTHQUAKE_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       4.4\n",
            "1       5.2\n",
            "2       4.8\n",
            "3       4.4\n",
            "4       5.7\n",
            "       ... \n",
            "6569    4.5\n",
            "6570    4.0\n",
            "6571    4.4\n",
            "6572    4.1\n",
            "6573    4.0\n",
            "Name: Magnitude, Length: 6574, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"datasets/eq.csv\")\n",
        "training_set = data.iloc[:, 4]\n",
        "print(training_set)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Gradients"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Calculating the value of_ $\\frac{dx}{dt}$, _and_ $\\frac{d^2x}{dt^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.9\n",
            "1       0.8\n",
            "2      -0.4\n",
            "3      -0.4\n",
            "4       1.3\n",
            "5      -1.4\n",
            "       ... \n",
            "6569    0.4\n",
            "6570   -0.5\n",
            "6571    0.4\n",
            "6572   -0.3\n",
            "6573   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "2      -1.200000e+00\n",
            "3       8.881784e-16\n",
            "4       1.700000e+00\n",
            "5      -2.700000e+00\n",
            "6       1.100000e+00\n",
            "            ...     \n",
            "6569    1.500000e+00\n",
            "6570   -9.000000e-01\n",
            "6571    9.000000e-01\n",
            "6572   -7.000000e-01\n",
            "6573    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "t_diff = 1 # Daily Data\n",
        "print(training_set.max())\n",
        "gradient_t = (training_set.diff()/t_diff).iloc[1:]\n",
        "print(gradient_t)\n",
        "gradient_tt = (gradient_t.diff()/t_diff).iloc[1:]\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       0.8\n",
            "1      -0.4\n",
            "2      -0.4\n",
            "3       1.3\n",
            "4      -1.4\n",
            "       ... \n",
            "6568    0.4\n",
            "6569   -0.5\n",
            "6570    0.4\n",
            "6571   -0.3\n",
            "6572   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "0      -1.200000e+00\n",
            "1       8.881784e-16\n",
            "2       1.700000e+00\n",
            "3      -2.700000e+00\n",
            "4       1.100000e+00\n",
            "            ...     \n",
            "6567    1.500000e+00\n",
            "6568   -9.000000e-01\n",
            "6569    9.000000e-01\n",
            "6570   -7.000000e-01\n",
            "6571    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "training_set = training_set.reset_index(drop=True)\n",
        "gradient_t = gradient_t.reset_index(drop=True)\n",
        "gradient_tt = gradient_tt.reset_index(drop=True)\n",
        "print(gradient_t)\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6573,)\n",
            "()\n"
          ]
        }
      ],
      "source": [
        "print(gradient_t.shape)\n",
        "print(training_set.shape[:-1])\n",
        "df = pd.concat((training_set[:-1], gradient_t), axis=1)\n",
        "gradient_tt.columns = [\"grad_tt\"]\n",
        "df = pd.concat((df[:-1], gradient_tt), axis=1)\n",
        "df.columns = ['y_t', 'grad_t', 'grad_tt']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-5esyHu5aFvg"
      },
      "source": [
        "## Plot of the External Forcing from Chaotic Differential Equation (_Duffing Equation_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "ym4xWUUxaFvg",
        "outputId": "45058d71-6952-4a40-afa5-84c2f42197b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "a=0.5\n",
        "b=1\n",
        "c=3\n",
        "d=1\n",
        "L = df.iloc[:, 2] -c + d*df.iloc[:, 1] - a*df.iloc[:, 0] - b*df.iloc[:, 0]**3\n",
        "L.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEywnwaFvh"
      },
      "source": [
        "## Preprocessing the data into supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6V9dXqzdaFvh"
      },
      "outputs": [],
      "source": [
        "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "      if i == 0:\n",
        "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "      else:\n",
        "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "       agg.dropna(inplace=True)\n",
        "    return agg    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI8Yfkw6oA0l",
        "outputId": "01d7b72a-3934-4f62-ca10-27be3fc0310c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['var1(t-10)', 'var2(t-10)', 'var3(t-10)', 'var1(t-9)', 'var2(t-9)',\n",
              "       'var3(t-9)', 'var1(t-8)', 'var2(t-8)', 'var3(t-8)', 'var1(t-7)',\n",
              "       ...\n",
              "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
              "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
              "       'var2(t+364)', 'var3(t+364)'],\n",
              "      dtype='object', length=1125)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "dat.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrzSrT1HnyfH",
        "outputId": "f37830fc-a93f-4216-e5d6-8e44bb1f83ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    var1(t-10)  var1(t-9)  var1(t-8)  var1(t-7)  var1(t-6)  var1(t-5)  \\\n",
            "10         4.4        5.2        4.8        4.4        5.7        4.3   \n",
            "11         5.2        4.8        4.4        5.7        4.3        4.0   \n",
            "12         4.8        4.4        5.7        4.3        4.0        4.8   \n",
            "13         4.4        5.7        4.3        4.0        4.8        5.2   \n",
            "14         5.7        4.3        4.0        4.8        5.2        5.1   \n",
            "\n",
            "    var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)  ...  var3(t+361)  var1(t+362)  \\\n",
            "10        4.0        4.8        5.2        5.1  ...         -1.3          5.7   \n",
            "11        4.8        5.2        5.1        4.7  ...          1.6          4.9   \n",
            "12        5.2        5.1        4.7        4.6  ...         -1.1          5.7   \n",
            "13        5.1        4.7        4.6        4.4  ...          0.1          5.4   \n",
            "14        4.7        4.6        4.4        4.7  ...          0.1          5.2   \n",
            "\n",
            "    var2(t+362)  var3(t+362)  var1(t+363)  var2(t+363)  var3(t+363)  \\\n",
            "10         -0.8          1.6          4.9          0.8         -1.1   \n",
            "11          0.8         -1.1          5.7         -0.3          0.1   \n",
            "12         -0.3          0.1          5.4         -0.2          0.1   \n",
            "13         -0.2          0.1          5.2         -0.1          0.2   \n",
            "14         -0.1          0.2          5.1          0.1          0.1   \n",
            "\n",
            "    var1(t+364)  var2(t+364)  var3(t+364)  \n",
            "10          5.7         -0.3          0.1  \n",
            "11          5.4         -0.2          0.1  \n",
            "12          5.2         -0.1          0.2  \n",
            "13          5.1          0.1          0.1  \n",
            "14          5.2          0.2         -0.1  \n",
            "\n",
            "[5 rows x 1105 columns]\n",
            "Index(['var1(t-10)', 'var1(t-9)', 'var1(t-8)', 'var1(t-7)', 'var1(t-6)',\n",
            "       'var1(t-5)', 'var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)',\n",
            "       ...\n",
            "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
            "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
            "       'var2(t+364)', 'var3(t+364)'],\n",
            "      dtype='object', length=1105)\n"
          ]
        }
      ],
      "source": [
        "data = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "data.drop(['var2(t-10)', 'var3(t-10)', 'var2(t-9)', 'var3(t-9)', 'var2(t-8)',\n",
        "       'var3(t-8)', 'var2(t-7)', 'var3(t-7)', 'var2(t-6)', 'var3(t-6)',\n",
        "       'var2(t-5)', 'var3(t-5)', 'var2(t-4)', 'var3(t-4)', 'var2(t-2)',\n",
        "       'var3(t-2)', 'var2(t-1)', 'var3(t-1)','var2(t-3)', 'var3(t-3)'], axis = 1, inplace = True)#,18,19\n",
        "print(data.head())\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKb5l_gUaFvi"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVOndQpQaFvi",
        "outputId": "56d94a79-1d05-4ff7-d70e-e2d8d00c83c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4958, 1, 1096) (4958, 9) (1240, 1, 1096) (1240, 9)\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(data) * 0.8)\n",
        "test_size = len(data) - train_size\n",
        "train_1 = np.array(data[0:train_size])\n",
        "test_1 = np.array(data[train_size:len(data)])\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train = scaler.fit_transform(train_1)\n",
        "test = scaler.transform(test_1)\n",
        "trainY = train[:,-9:]\n",
        "trainX = train[:,:-9]\n",
        "testY = test[:,-9:]\n",
        "testX = test[:,:-9]\n",
        "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = testX.reshape((testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-D_j8UaFvj"
      },
      "source": [
        "## Defining the Physical Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Jw7vitLaFvj"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.5, name=\"alpha\", trainable=True, dtype=tf.float32)\n",
        "b = tf.Variable(1, name=\"beta\", trainable=True, dtype=tf.float32)\n",
        "c = tf.Variable(3, name=\"gamma\", trainable=True, dtype=tf.float32)\n",
        "d = tf.Variable(1, name=\"delta\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def phys(y_pred, y_true):\n",
        "    return mean_absolute_error((y_true[:, 2] -c + d*y_true[:, 1] - a*y_true[:, 0] - b*y_true[:, 0]**3), (y_pred[:, 2] -c + d*y_pred[:, 1] - a*y_pred[:, 0] - b*y_pred[:, 0]**3))\n",
        "\n",
        "def phys2(y_pred, y_real):\n",
        "    pred = y_pred[2:]-2*y_pred[1:-1]-y_pred[:-2] -c + d*(y_pred[1:-1]-y_pred[:-2]) - a*y_pred[:-2] - b*y_pred[:-2]**3\n",
        "    real = y_real[2:]-2*y_real[1:-1]-y_real[:-2] -c + d*(y_real[1:-1]-y_real[:-2]) - a*y_real[:-2] - b*y_real[:-2]**3\n",
        "    return(mean_absolute_error(pred, real))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--1LVbHOBSIy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "874xZ-_u7X_s",
        "outputId": "883c11bb-1fe0-48af-e119-65529c186443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "78/78 [==============================] - 3s 12ms/step - loss: 0.0220 - val_loss: 0.0087\n",
            "Epoch 2/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 3/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0084\n",
            "Epoch 4/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0084\n",
            "Epoch 5/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0083\n",
            "Epoch 6/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0083\n",
            "Epoch 7/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0083\n",
            "Epoch 8/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0082\n",
            "Epoch 9/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0156 - val_loss: 0.0083\n",
            "Epoch 10/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0082\n",
            "Epoch 11/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0084\n",
            "Epoch 12/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0082\n",
            "Epoch 13/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0085\n",
            "Epoch 14/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0084\n",
            "Epoch 15/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0086\n",
            "Epoch 16/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0086\n",
            "Epoch 17/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0082\n",
            "Epoch 18/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0140 - val_loss: 0.0085\n",
            "Epoch 19/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0080\n",
            "Epoch 20/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0080\n",
            "Epoch 21/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0080\n",
            "Epoch 22/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0080\n",
            "Epoch 23/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0079\n",
            "Epoch 24/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0079\n",
            "Epoch 25/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0079\n",
            "Epoch 26/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0078\n",
            "Epoch 27/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0078\n",
            "Epoch 28/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0078\n",
            "Epoch 29/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0078\n",
            "Epoch 30/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0078\n",
            "Epoch 31/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0077\n",
            "Epoch 32/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0078\n",
            "Epoch 33/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0078\n",
            "Epoch 34/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0077\n",
            "Epoch 35/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0077\n",
            "Epoch 36/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0077\n",
            "Epoch 37/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0077\n",
            "Epoch 38/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0077\n",
            "Epoch 39/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0076\n",
            "Epoch 40/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0076\n",
            "Epoch 41/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0115 - val_loss: 0.0075\n",
            "Epoch 42/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0075\n",
            "Epoch 43/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0075\n",
            "Epoch 44/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0074\n",
            "Epoch 45/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0075\n",
            "Epoch 46/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0074\n",
            "Epoch 47/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0074\n",
            "Epoch 48/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0073\n",
            "Epoch 49/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0073\n",
            "Epoch 50/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0073\n",
            "Epoch 51/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0072\n",
            "Epoch 52/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0072\n",
            "Epoch 53/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0072\n",
            "Epoch 54/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0071\n",
            "Epoch 55/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0071\n",
            "Epoch 56/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0069\n",
            "Epoch 57/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0069\n",
            "Epoch 58/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0069\n",
            "Epoch 59/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0068\n",
            "Epoch 60/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0068\n",
            "Epoch 61/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0068\n",
            "Epoch 62/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0068\n",
            "Epoch 63/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0067\n",
            "Epoch 64/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0067\n",
            "Epoch 65/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0067\n",
            "Epoch 66/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0067\n",
            "Epoch 67/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0067\n",
            "Epoch 68/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0066\n",
            "Epoch 69/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0066\n",
            "Epoch 70/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0065\n",
            "Epoch 71/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0065\n",
            "Epoch 72/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0091 - val_loss: 0.0065\n",
            "Epoch 73/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0064\n",
            "Epoch 74/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0064\n",
            "Epoch 75/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0064\n",
            "Epoch 76/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0063\n",
            "Epoch 77/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0063\n",
            "Epoch 78/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0063\n",
            "Epoch 79/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0062\n",
            "Epoch 80/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0062\n",
            "Epoch 81/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0062\n",
            "Epoch 82/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0061\n",
            "Epoch 83/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0060\n",
            "Epoch 84/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0061\n",
            "Epoch 85/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0082 - val_loss: 0.0060\n",
            "Epoch 86/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0059\n",
            "Epoch 87/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0059\n",
            "Epoch 88/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0058\n",
            "Epoch 89/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0060\n",
            "Epoch 90/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0059\n",
            "Epoch 91/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0059\n",
            "Epoch 92/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0059\n",
            "Epoch 93/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0058\n",
            "Epoch 94/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0058\n",
            "Epoch 95/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0058\n",
            "Epoch 96/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0058\n",
            "Epoch 97/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0057\n",
            "Epoch 98/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0058\n",
            "Epoch 99/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 100/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 101/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 102/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0056\n",
            "Epoch 103/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0077 - val_loss: 0.0057\n",
            "Epoch 104/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0075 - val_loss: 0.0058\n",
            "Epoch 105/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 106/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 107/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0058\n",
            "Epoch 108/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0057\n",
            "Epoch 109/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0057\n",
            "Epoch 110/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0056\n",
            "Epoch 111/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0056\n",
            "Epoch 112/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0080 - val_loss: 0.0055\n",
            "Epoch 113/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0082 - val_loss: 0.0055\n",
            "Epoch 114/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0054\n",
            "Epoch 115/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0055\n",
            "Epoch 116/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0055\n",
            "Epoch 117/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0056\n",
            "Epoch 118/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0063\n",
            "Epoch 119/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0084\n",
            "Epoch 120/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0173 - val_loss: 0.0082\n",
            "Epoch 121/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0083\n",
            "Epoch 122/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0081\n",
            "Epoch 123/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0083\n",
            "Epoch 124/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0155 - val_loss: 0.0078\n",
            "Epoch 125/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0074\n",
            "Epoch 126/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0054\n",
            "Epoch 127/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0087 - val_loss: 0.0056\n",
            "Epoch 128/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0056\n",
            "Epoch 129/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0056\n",
            "Epoch 130/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0095 - val_loss: 0.0055\n",
            "Epoch 131/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0056\n",
            "Epoch 132/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0059\n",
            "Epoch 133/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0078\n",
            "Epoch 134/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0066\n",
            "Epoch 135/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0080\n",
            "Epoch 136/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0058\n",
            "Epoch 137/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0078\n",
            "Epoch 138/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.0065\n",
            "Epoch 139/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0057\n",
            "Epoch 140/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0058\n",
            "Epoch 141/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0062\n",
            "Epoch 142/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0074\n",
            "Epoch 143/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0139 - val_loss: 0.0083\n",
            "Epoch 144/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 145/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.0065\n",
            "Epoch 146/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0078\n",
            "Epoch 147/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0131 - val_loss: 0.0083\n",
            "Epoch 148/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0061\n",
            "Epoch 149/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0080\n",
            "Epoch 150/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0082\n",
            "Epoch 151/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0083\n",
            "Epoch 152/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0070\n",
            "Epoch 153/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0061\n",
            "Epoch 154/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0060\n",
            "Epoch 155/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0058\n",
            "Epoch 156/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0056\n",
            "Epoch 157/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0055\n",
            "Epoch 158/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0054\n",
            "Epoch 159/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0054\n",
            "Epoch 160/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0054\n",
            "Epoch 161/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0088 - val_loss: 0.0054\n",
            "Epoch 162/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0055\n",
            "Epoch 163/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0054\n",
            "Epoch 164/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0057\n",
            "Epoch 165/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0080 - val_loss: 0.0056\n",
            "Epoch 166/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0055\n",
            "Epoch 167/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0055\n",
            "Epoch 168/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0053\n",
            "Epoch 169/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0052\n",
            "Epoch 170/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0054\n",
            "Epoch 171/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0080 - val_loss: 0.0052\n",
            "Epoch 172/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0052\n",
            "Epoch 173/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0049\n",
            "Epoch 174/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0051\n",
            "Epoch 175/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0050\n",
            "Epoch 176/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0050\n",
            "Epoch 177/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0049\n",
            "Epoch 178/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0049\n",
            "Epoch 179/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0049\n",
            "Epoch 180/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0048\n",
            "Epoch 181/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0074 - val_loss: 0.0048\n",
            "Epoch 182/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0074 - val_loss: 0.0047\n",
            "Epoch 183/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0073 - val_loss: 0.0047\n",
            "Epoch 184/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0072 - val_loss: 0.0046\n",
            "Epoch 185/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0046\n",
            "Epoch 186/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 0.0046\n",
            "Epoch 187/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 0.0045\n",
            "Epoch 188/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0067 - val_loss: 0.0044\n",
            "Epoch 189/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0066 - val_loss: 0.0044\n",
            "Epoch 190/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0065 - val_loss: 0.0043\n",
            "Epoch 191/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0064 - val_loss: 0.0043\n",
            "Epoch 192/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0063 - val_loss: 0.0042\n",
            "Epoch 193/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.0041\n",
            "Epoch 194/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0062 - val_loss: 0.0041\n",
            "Epoch 195/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0061 - val_loss: 0.0040\n",
            "Epoch 196/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0060 - val_loss: 0.0040\n",
            "Epoch 197/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0059 - val_loss: 0.0039\n",
            "Epoch 198/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.0039\n",
            "Epoch 199/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0037\n",
            "Epoch 200/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0060 - val_loss: 0.0037\n",
            "Epoch 201/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.0038\n",
            "Epoch 202/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0037\n",
            "Epoch 203/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0037\n",
            "Epoch 204/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0037\n",
            "Epoch 205/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0051 - val_loss: 0.0036\n",
            "Epoch 206/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.0034\n",
            "Epoch 207/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0052 - val_loss: 0.0034\n",
            "Epoch 208/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0052 - val_loss: 0.0035\n",
            "Epoch 209/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0047 - val_loss: 0.0034\n",
            "Epoch 210/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0032\n",
            "Epoch 211/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.0035\n",
            "Epoch 212/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0032\n",
            "Epoch 213/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0031\n",
            "Epoch 214/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0029\n",
            "Epoch 215/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0030\n",
            "Epoch 216/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0028\n",
            "Epoch 217/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0030\n",
            "Epoch 218/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0029\n",
            "Epoch 219/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0029\n",
            "Epoch 220/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0028\n",
            "Epoch 221/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0028\n",
            "Epoch 222/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0027\n",
            "Epoch 223/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 0.0025\n",
            "Epoch 224/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0027\n",
            "Epoch 225/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 226/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 227/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 228/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 229/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 230/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 231/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 232/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 233/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 234/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 235/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 236/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 237/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 238/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0018\n",
            "Epoch 239/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0017\n",
            "Epoch 240/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0027 - val_loss: 0.0016\n",
            "Epoch 241/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0025 - val_loss: 0.0017\n",
            "Epoch 242/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 243/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0017\n",
            "Epoch 244/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 245/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 246/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.0015\n",
            "Epoch 247/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 248/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0012\n",
            "Epoch 249/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 250/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0016\n",
            "Epoch 251/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.0017\n",
            "Epoch 252/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.0014\n",
            "Epoch 253/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0014\n",
            "Epoch 254/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0016\n",
            "Epoch 255/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0068 - val_loss: 0.0018\n",
            "Epoch 256/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0027\n",
            "Epoch 257/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0172 - val_loss: 0.0084\n",
            "Epoch 258/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0172 - val_loss: 0.0082\n",
            "Epoch 259/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0081\n",
            "Epoch 260/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0079\n",
            "Epoch 261/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0063\n",
            "Epoch 262/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0023\n",
            "Epoch 263/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 264/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.0081\n",
            "Epoch 265/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0080\n",
            "Epoch 266/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0068\n",
            "Epoch 267/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0026\n",
            "Epoch 268/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0176 - val_loss: 0.0082\n",
            "Epoch 269/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0083\n",
            "Epoch 270/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0082\n",
            "Epoch 271/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0081\n",
            "Epoch 272/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0070\n",
            "Epoch 273/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.0023\n",
            "Epoch 274/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0083\n",
            "Epoch 275/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0082\n",
            "Epoch 276/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0079\n",
            "Epoch 277/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0048\n",
            "Epoch 278/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0160 - val_loss: 0.0064\n",
            "Epoch 279/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0027\n",
            "Epoch 280/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0175 - val_loss: 0.0083\n",
            "Epoch 281/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0081\n",
            "Epoch 282/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0082\n",
            "Epoch 283/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0082\n",
            "Epoch 284/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0082\n",
            "Epoch 285/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0077\n",
            "Epoch 286/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0030\n",
            "Epoch 287/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0163 - val_loss: 0.0082\n",
            "Epoch 288/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0080\n",
            "Epoch 289/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0082\n",
            "Epoch 290/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0077\n",
            "Epoch 291/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0155 - val_loss: 0.0073\n",
            "Epoch 292/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0029\n",
            "Epoch 293/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0080\n",
            "Epoch 294/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0069\n",
            "Epoch 295/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0028\n",
            "Epoch 296/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0180 - val_loss: 0.0083\n",
            "Epoch 297/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0082\n",
            "Epoch 298/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0074\n",
            "Epoch 299/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0073\n",
            "Epoch 300/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0073\n",
            "Epoch 301/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0076\n",
            "Epoch 302/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0077\n",
            "Epoch 303/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0077\n",
            "Epoch 304/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0077\n",
            "Epoch 305/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0076\n",
            "Epoch 306/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0124 - val_loss: 0.0074\n",
            "Epoch 307/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0057\n",
            "Epoch 308/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0025\n",
            "Epoch 309/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0158 - val_loss: 0.0082\n",
            "Epoch 310/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0164 - val_loss: 0.0075\n",
            "Epoch 311/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0056\n",
            "Epoch 312/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0080 - val_loss: 0.0025\n",
            "Epoch 313/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0145 - val_loss: 0.0047\n",
            "Epoch 314/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0167 - val_loss: 0.0084\n",
            "Epoch 315/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0084\n",
            "Epoch 316/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0170 - val_loss: 0.0084\n",
            "Epoch 317/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.0086\n",
            "Epoch 318/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0082\n",
            "Epoch 319/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0168 - val_loss: 0.0083\n",
            "Epoch 320/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0082\n",
            "Epoch 321/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0167 - val_loss: 0.0077\n",
            "Epoch 322/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0025\n",
            "Epoch 323/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0074 - val_loss: 0.0020\n",
            "Epoch 324/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0029\n",
            "Epoch 325/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0039\n",
            "Epoch 326/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0083\n",
            "Epoch 327/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0079\n",
            "Epoch 328/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0070\n",
            "Epoch 329/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0070\n",
            "Epoch 330/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0071\n",
            "Epoch 331/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0072\n",
            "Epoch 332/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0072\n",
            "Epoch 333/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0070\n",
            "Epoch 334/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0069\n",
            "Epoch 335/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0069\n",
            "Epoch 336/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0068\n",
            "Epoch 337/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0068\n",
            "Epoch 338/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0066\n",
            "Epoch 339/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0066\n",
            "Epoch 340/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0065\n",
            "Epoch 341/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0065\n",
            "Epoch 342/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0096 - val_loss: 0.0050\n",
            "Epoch 343/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0025\n",
            "Epoch 344/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0065\n",
            "Epoch 345/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0062\n",
            "Epoch 346/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0063\n",
            "Epoch 347/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0060\n",
            "Epoch 348/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0080 - val_loss: 0.0027\n",
            "Epoch 349/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0063 - val_loss: 0.0025\n",
            "Epoch 350/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0027\n",
            "Epoch 351/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0026\n",
            "Epoch 352/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0029\n",
            "Epoch 353/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0089 - val_loss: 0.0030\n",
            "Epoch 354/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0055\n",
            "Epoch 355/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0030\n",
            "Epoch 356/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0164 - val_loss: 0.0080\n",
            "Epoch 357/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0061\n",
            "Epoch 358/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0091 - val_loss: 0.0057\n",
            "Epoch 359/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0089 - val_loss: 0.0058\n",
            "Epoch 360/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0057\n",
            "Epoch 361/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0081 - val_loss: 0.0055\n",
            "Epoch 362/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0078 - val_loss: 0.0054\n",
            "Epoch 363/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0053\n",
            "Epoch 364/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0051\n",
            "Epoch 365/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0075 - val_loss: 0.0050\n",
            "Epoch 366/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 0.0049\n",
            "Epoch 367/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0048\n",
            "Epoch 368/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0085 - val_loss: 0.0044\n",
            "Epoch 369/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0074 - val_loss: 0.0027\n",
            "Epoch 370/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0045\n",
            "Epoch 371/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0033\n",
            "Epoch 372/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0031\n",
            "Epoch 373/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0073 - val_loss: 0.0031\n",
            "Epoch 374/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0030\n",
            "Epoch 375/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0139 - val_loss: 0.0068\n",
            "Epoch 376/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0029\n",
            "Epoch 377/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0163 - val_loss: 0.0083\n",
            "Epoch 378/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 379/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0082\n",
            "Epoch 380/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0081\n",
            "Epoch 381/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0158 - val_loss: 0.0082\n",
            "Epoch 382/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0080\n",
            "Epoch 383/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.0049\n",
            "Epoch 384/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0030\n",
            "Epoch 385/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0065 - val_loss: 0.0034\n",
            "Epoch 386/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0083 - val_loss: 0.0031\n",
            "Epoch 387/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0029\n",
            "Epoch 388/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0040\n",
            "Epoch 389/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0031\n",
            "Epoch 390/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0059\n",
            "Epoch 391/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0032\n",
            "Epoch 392/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0056 - val_loss: 0.0033\n",
            "Epoch 393/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0098 - val_loss: 0.0034\n",
            "Epoch 394/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0058 - val_loss: 0.0031\n",
            "Epoch 395/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0041\n",
            "Epoch 396/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0056 - val_loss: 0.0028\n",
            "Epoch 397/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0061 - val_loss: 0.0036\n",
            "Epoch 398/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0090 - val_loss: 0.0033\n",
            "Epoch 399/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0079\n",
            "Epoch 400/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0038\n",
            "Epoch 401/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0083\n",
            "Epoch 402/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0082\n",
            "Epoch 403/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0054\n",
            "Epoch 404/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0049\n",
            "Epoch 405/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0066 - val_loss: 0.0042\n",
            "Epoch 406/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 0.0029\n",
            "Epoch 407/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 0.0029\n",
            "Epoch 408/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0032\n",
            "Epoch 409/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0044\n",
            "Epoch 410/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0028\n",
            "Epoch 411/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0056 - val_loss: 0.0028\n",
            "Epoch 412/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0029\n",
            "Epoch 413/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0030\n",
            "Epoch 414/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0048 - val_loss: 0.0031\n",
            "Epoch 415/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0031\n",
            "Epoch 416/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0029\n",
            "Epoch 417/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0025\n",
            "Epoch 418/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0028\n",
            "Epoch 419/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.0027\n",
            "Epoch 420/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0054 - val_loss: 0.0026\n",
            "Epoch 421/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0024\n",
            "Epoch 422/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0024\n",
            "Epoch 423/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0023\n",
            "Epoch 424/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0023\n",
            "Epoch 425/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0022\n",
            "Epoch 426/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0022\n",
            "Epoch 427/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0022\n",
            "Epoch 428/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0022\n",
            "Epoch 429/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 430/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 431/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 432/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 433/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0020\n",
            "Epoch 434/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0019\n",
            "Epoch 435/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 436/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0018\n",
            "Epoch 437/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 438/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0017\n",
            "Epoch 439/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 440/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0016\n",
            "Epoch 441/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0016\n",
            "Epoch 442/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 443/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 444/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0015\n",
            "Epoch 445/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 446/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 447/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 448/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 449/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 450/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 451/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 452/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 453/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 454/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 455/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 456/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 457/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 458/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 459/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 460/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0014\n",
            "Epoch 461/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 462/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 463/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0013\n",
            "Epoch 464/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0013\n",
            "Epoch 465/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0012\n",
            "Epoch 466/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0013\n",
            "Epoch 467/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 468/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 469/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 470/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 471/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.0013\n",
            "Epoch 472/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.0013\n",
            "Epoch 473/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0022 - val_loss: 0.0013\n",
            "Epoch 474/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0013\n",
            "Epoch 475/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0012\n",
            "Epoch 476/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0010\n",
            "Epoch 477/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0010\n",
            "Epoch 478/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 9.9780e-04\n",
            "Epoch 479/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 9.9252e-04\n",
            "Epoch 480/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0010\n",
            "Epoch 481/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0011\n",
            "Epoch 482/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0064 - val_loss: 9.7899e-04\n",
            "Epoch 483/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0082 - val_loss: 0.0011\n",
            "Epoch 484/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0012\n",
            "Epoch 485/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0069 - val_loss: 0.0012\n",
            "Epoch 486/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0062 - val_loss: 0.0014\n",
            "Epoch 487/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0054 - val_loss: 0.0013\n",
            "Epoch 488/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0013\n",
            "Epoch 489/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0012\n",
            "Epoch 490/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0045 - val_loss: 0.0012\n",
            "Epoch 491/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0044 - val_loss: 0.0012\n",
            "Epoch 492/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0012\n",
            "Epoch 493/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 494/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 495/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0013\n",
            "Epoch 496/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0013\n",
            "Epoch 497/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0013\n",
            "Epoch 498/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 499/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 500/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0014\n"
          ]
        }
      ],
      "source": [
        "a = tf.Variable(0.1, name=\"alpha\", trainable=True, dtype=tf.float32)\n",
        "b = tf.Variable(0.05, name=\"beta\", trainable=True, dtype=tf.float32)\n",
        "c = tf.Variable(1.1, name=\"gamma\", trainable=True, dtype=tf.float32)\n",
        "d = tf.Variable(0.1, name=\"delta\", trainable=True, dtype=tf.float32)\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    squared_difference = tf.square(y_true[:, 0] - y_pred[:, 0])\n",
        "    squared_difference2 = tf.square(y_true[:, 2]-y_pred[:, 2])\n",
        "    squared_difference1 = tf.square(y_true[:, 1]-y_pred[:, 1])\n",
        "    squared_difference3 = tf.square(y_pred[:, 2] -c + d*y_pred[:, 1] - a*y_pred[:, 0] - b*y_pred[:, 0]**3)\n",
        "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(Dense(9))\n",
        "model.compile(loss=loss_fn, optimizer='adam')\n",
        "history = model.fit(trainX, trainY, epochs=500, batch_size=64, validation_data=(testX, testY), shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 1s 2ms/step\n",
            "(1240, 9)\n",
            "(1240, 1096)\n",
            "Test RMSE: 2.865\n",
            "Test MAE: 2.842\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "yhat = model.predict(testX)\n",
        "print(yhat.shape)\n",
        "testX = testX.reshape((testX.shape[0], testX.shape[2]))\n",
        "print(testX.shape)\n",
        "inv_yhat = np.concatenate((testX, yhat), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat1 = inv_yhat[:, -3:]\n",
        "inv_yhat = inv_yhat[:, -3]\n",
        "inv_y = np.concatenate((testX, testY), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y1 = inv_y[:, -3:]\n",
        "inv_y = inv_y[:, -3]\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "mae = mean_absolute_error(inv_y, inv_yhat)\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
