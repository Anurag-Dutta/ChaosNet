{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xSItPJipBaZ5"
      },
      "source": [
        "## Gathering Dependencies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Importing Required Libraries_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-6LN-zXiLcM",
        "outputId": "1a821417-b2e6-4bf3-ad0b-494bb85bea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hampel in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.0.5)\n",
            "Requirement already satisfied: pandas in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.3.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from hampel) (1.21.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pandas->hampel) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anurag dutta\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from python-dateutil>=2.7.3->pandas->hampel) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Anurag Dutta\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install hampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "By_d9uXpaFvZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from hampel import hampel\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "from numpy import array"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_TURKEY EARTHQUAKE_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       4.4\n",
            "1       5.2\n",
            "2       4.8\n",
            "3       4.4\n",
            "4       5.7\n",
            "       ... \n",
            "6569    4.5\n",
            "6570    4.0\n",
            "6571    4.4\n",
            "6572    4.1\n",
            "6573    4.0\n",
            "Name: Magnitude, Length: 6574, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"datasets/eq.csv\")\n",
        "training_set = data.iloc[:, 4]\n",
        "print(training_set)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing the Gradients"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Calculating the value of_ $\\frac{dx}{dt}$, _and_ $\\frac{d^2x}{dt^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.9\n",
            "1       0.8\n",
            "2      -0.4\n",
            "3      -0.4\n",
            "4       1.3\n",
            "5      -1.4\n",
            "       ... \n",
            "6569    0.4\n",
            "6570   -0.5\n",
            "6571    0.4\n",
            "6572   -0.3\n",
            "6573   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "2      -1.200000e+00\n",
            "3       8.881784e-16\n",
            "4       1.700000e+00\n",
            "5      -2.700000e+00\n",
            "6       1.100000e+00\n",
            "            ...     \n",
            "6569    1.500000e+00\n",
            "6570   -9.000000e-01\n",
            "6571    9.000000e-01\n",
            "6572   -7.000000e-01\n",
            "6573    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "t_diff = 1 # Daily Data\n",
        "print(training_set.max())\n",
        "gradient_t = (training_set.diff()/t_diff).iloc[1:]\n",
        "print(gradient_t)\n",
        "gradient_tt = (gradient_t.diff()/t_diff).iloc[1:]\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0       0.8\n",
            "1      -0.4\n",
            "2      -0.4\n",
            "3       1.3\n",
            "4      -1.4\n",
            "       ... \n",
            "6568    0.4\n",
            "6569   -0.5\n",
            "6570    0.4\n",
            "6571   -0.3\n",
            "6572   -0.1\n",
            "Name: Magnitude, Length: 6573, dtype: float64\n",
            "0      -1.200000e+00\n",
            "1       8.881784e-16\n",
            "2       1.700000e+00\n",
            "3      -2.700000e+00\n",
            "4       1.100000e+00\n",
            "            ...     \n",
            "6567    1.500000e+00\n",
            "6568   -9.000000e-01\n",
            "6569    9.000000e-01\n",
            "6570   -7.000000e-01\n",
            "6571    2.000000e-01\n",
            "Name: Magnitude, Length: 6572, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "training_set = training_set.reset_index(drop=True)\n",
        "gradient_t = gradient_t.reset_index(drop=True)\n",
        "gradient_tt = gradient_tt.reset_index(drop=True)\n",
        "print(gradient_t)\n",
        "print(gradient_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6573,)\n",
            "()\n"
          ]
        }
      ],
      "source": [
        "print(gradient_t.shape)\n",
        "print(training_set.shape[:-1])\n",
        "df = pd.concat((training_set[:-1], gradient_t), axis=1)\n",
        "gradient_tt.columns = [\"grad_tt\"]\n",
        "df = pd.concat((df[:-1], gradient_tt), axis=1)\n",
        "df.columns = ['y_t', 'grad_t', 'grad_tt']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-5esyHu5aFvg"
      },
      "source": [
        "## Plot of the External Forcing from Chaotic Differential Equation (_Lienard Systems_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "ym4xWUUxaFvg",
        "outputId": "45058d71-6952-4a40-afa5-84c2f42197b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "a = 0.45\n",
        "b = 0.5 #1.0\n",
        "c = -0.5 #-1.68\n",
        "L = df.iloc[:,2] + a*df.iloc[:,0]*df.iloc[:,1] +c*df.iloc[:,0] +b* df.iloc[:,0]**3\n",
        "L.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9VyEywnwaFvh"
      },
      "source": [
        "## Preprocessing the data into supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6V9dXqzdaFvh"
      },
      "outputs": [],
      "source": [
        "def Supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "      cols.append(df.shift(-i))\n",
        "      if i == 0:\n",
        "        names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "      else:\n",
        "        names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "       agg.dropna(inplace=True)\n",
        "    return agg    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI8Yfkw6oA0l",
        "outputId": "01d7b72a-3934-4f62-ca10-27be3fc0310c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['var1(t-10)', 'var2(t-10)', 'var3(t-10)', 'var1(t-9)', 'var2(t-9)',\n",
              "       'var3(t-9)', 'var1(t-8)', 'var2(t-8)', 'var3(t-8)', 'var1(t-7)',\n",
              "       ...\n",
              "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
              "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
              "       'var2(t+364)', 'var3(t+364)'],\n",
              "      dtype='object', length=1125)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dat = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "dat.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrzSrT1HnyfH",
        "outputId": "f37830fc-a93f-4216-e5d6-8e44bb1f83ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    var1(t-10)  var1(t-9)  var1(t-8)  var1(t-7)  var1(t-6)  var1(t-5)  \\\n",
            "10         4.4        5.2        4.8        4.4        5.7        4.3   \n",
            "11         5.2        4.8        4.4        5.7        4.3        4.0   \n",
            "12         4.8        4.4        5.7        4.3        4.0        4.8   \n",
            "13         4.4        5.7        4.3        4.0        4.8        5.2   \n",
            "14         5.7        4.3        4.0        4.8        5.2        5.1   \n",
            "\n",
            "    var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)  ...  var3(t+361)  var1(t+362)  \\\n",
            "10        4.0        4.8        5.2        5.1  ...         -1.3          5.7   \n",
            "11        4.8        5.2        5.1        4.7  ...          1.6          4.9   \n",
            "12        5.2        5.1        4.7        4.6  ...         -1.1          5.7   \n",
            "13        5.1        4.7        4.6        4.4  ...          0.1          5.4   \n",
            "14        4.7        4.6        4.4        4.7  ...          0.1          5.2   \n",
            "\n",
            "    var2(t+362)  var3(t+362)  var1(t+363)  var2(t+363)  var3(t+363)  \\\n",
            "10         -0.8          1.6          4.9          0.8         -1.1   \n",
            "11          0.8         -1.1          5.7         -0.3          0.1   \n",
            "12         -0.3          0.1          5.4         -0.2          0.1   \n",
            "13         -0.2          0.1          5.2         -0.1          0.2   \n",
            "14         -0.1          0.2          5.1          0.1          0.1   \n",
            "\n",
            "    var1(t+364)  var2(t+364)  var3(t+364)  \n",
            "10          5.7         -0.3          0.1  \n",
            "11          5.4         -0.2          0.1  \n",
            "12          5.2         -0.1          0.2  \n",
            "13          5.1          0.1          0.1  \n",
            "14          5.2          0.2         -0.1  \n",
            "\n",
            "[5 rows x 1105 columns]\n",
            "Index(['var1(t-10)', 'var1(t-9)', 'var1(t-8)', 'var1(t-7)', 'var1(t-6)',\n",
            "       'var1(t-5)', 'var1(t-4)', 'var1(t-3)', 'var1(t-2)', 'var1(t-1)',\n",
            "       ...\n",
            "       'var3(t+361)', 'var1(t+362)', 'var2(t+362)', 'var3(t+362)',\n",
            "       'var1(t+363)', 'var2(t+363)', 'var3(t+363)', 'var1(t+364)',\n",
            "       'var2(t+364)', 'var3(t+364)'],\n",
            "      dtype='object', length=1105)\n"
          ]
        }
      ],
      "source": [
        "data = Supervised(df.values, n_in = 10, n_out = 365)\n",
        "data.drop(['var2(t-10)', 'var3(t-10)', 'var2(t-9)', 'var3(t-9)', 'var2(t-8)',\n",
        "       'var3(t-8)', 'var2(t-7)', 'var3(t-7)', 'var2(t-6)', 'var3(t-6)',\n",
        "       'var2(t-5)', 'var3(t-5)', 'var2(t-4)', 'var3(t-4)', 'var2(t-2)',\n",
        "       'var3(t-2)', 'var2(t-1)', 'var3(t-1)','var2(t-3)', 'var3(t-3)'], axis = 1, inplace = True)#,18,19\n",
        "print(data.head())\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKb5l_gUaFvi"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVOndQpQaFvi",
        "outputId": "56d94a79-1d05-4ff7-d70e-e2d8d00c83c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4958, 1, 1096) (4958, 9) (1240, 1, 1096) (1240, 9)\n"
          ]
        }
      ],
      "source": [
        "train_size = int(len(data) * 0.8)\n",
        "test_size = len(data) - train_size\n",
        "train_1 = np.array(data[0:train_size])\n",
        "test_1 = np.array(data[train_size:len(data)])\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train = scaler.fit_transform(train_1)\n",
        "test = scaler.transform(test_1)\n",
        "trainY = train[:,-9:]\n",
        "trainX = train[:,:-9]\n",
        "testY = test[:,-9:]\n",
        "testX = test[:,:-9]\n",
        "trainX = trainX.reshape((trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = testX.reshape((testX.shape[0], 1, testX.shape[1]))\n",
        "print(trainX.shape, trainY.shape, testX.shape, testY.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-D_j8UaFvj"
      },
      "source": [
        "## Defining the Physical Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Jw7vitLaFvj"
      },
      "outputs": [],
      "source": [
        "a = tf.Variable(0.45, name=\"a\", trainable=True, dtype=tf.float32)\n",
        "b = tf.Variable(0.5, name=\"b\", trainable=True, dtype=tf.float32)\n",
        "c = tf.Variable(-0.5, name=\"c\", trainable=True, dtype=tf.float32)\n",
        "def phys(y_pred, y_true):\n",
        "    return mean_absolute_error(y_true[:,2] + a*y_true[:,0]*y_true[:,1] +c*y_true[:,0] +b* y_true[:,0]**3, y_pred[:,2] + a*y_pred[:,0]*y_pred[:,1] +c*y_pred[:,0] +b* y_pred[:,0]**3)\n",
        "def phys2(y_pred, y_real):\n",
        "    pred = y_pred[2:]-2*y_pred[1:-1]-y_pred[:-2] +a*y_pred[:-2]*(y_pred[1:-1]-y_pred[:-2]) +c*y_pred[:-2] +b* y_pred[:-2]**3\n",
        "    real = y_real[2:]-2*y_real[1:-1]-y_real[:-2] +a*y_real[:-2]*(y_real[1:-1]-y_real[:-2]) +c*y_real[:-2] +b* y_real[:-2]**3\n",
        "    return(mean_absolute_error(pred,real))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--1LVbHOBSIy"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "874xZ-_u7X_s",
        "outputId": "883c11bb-1fe0-48af-e119-65529c186443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "78/78 [==============================] - 3s 10ms/step - loss: 0.0211 - val_loss: 0.0087\n",
            "Epoch 2/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0083\n",
            "Epoch 3/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0176 - val_loss: 0.0083\n",
            "Epoch 4/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0163 - val_loss: 0.0083\n",
            "Epoch 5/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0083\n",
            "Epoch 6/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0152 - val_loss: 0.0084\n",
            "Epoch 7/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0154 - val_loss: 0.0083\n",
            "Epoch 8/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0085\n",
            "Epoch 9/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0152 - val_loss: 0.0083\n",
            "Epoch 10/500\n",
            "78/78 [==============================] - 0s 3ms/step - loss: 0.0147 - val_loss: 0.0082\n",
            "Epoch 11/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0141 - val_loss: 0.0084\n",
            "Epoch 12/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0137 - val_loss: 0.0083\n",
            "Epoch 13/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0134 - val_loss: 0.0084\n",
            "Epoch 14/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0084\n",
            "Epoch 15/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0132 - val_loss: 0.0084\n",
            "Epoch 16/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0081\n",
            "Epoch 17/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0083\n",
            "Epoch 18/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0082\n",
            "Epoch 19/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0131 - val_loss: 0.0082\n",
            "Epoch 20/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0081\n",
            "Epoch 21/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0081\n",
            "Epoch 22/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0081\n",
            "Epoch 23/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0079\n",
            "Epoch 24/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0130 - val_loss: 0.0080\n",
            "Epoch 25/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0078\n",
            "Epoch 26/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0078\n",
            "Epoch 27/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0078\n",
            "Epoch 28/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0129 - val_loss: 0.0078\n",
            "Epoch 29/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0077\n",
            "Epoch 30/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0128 - val_loss: 0.0077\n",
            "Epoch 31/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0076\n",
            "Epoch 32/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0075\n",
            "Epoch 33/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0122 - val_loss: 0.0075\n",
            "Epoch 34/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0117 - val_loss: 0.0075\n",
            "Epoch 35/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0121 - val_loss: 0.0075\n",
            "Epoch 36/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0074\n",
            "Epoch 37/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0074\n",
            "Epoch 38/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0074\n",
            "Epoch 39/500\n",
            "78/78 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0074\n",
            "Epoch 40/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0073\n",
            "Epoch 41/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0073\n",
            "Epoch 42/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0073\n",
            "Epoch 43/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0116 - val_loss: 0.0073\n",
            "Epoch 44/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0073\n",
            "Epoch 45/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0073\n",
            "Epoch 46/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0072\n",
            "Epoch 47/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0072\n",
            "Epoch 48/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0072\n",
            "Epoch 49/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0071\n",
            "Epoch 50/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0070\n",
            "Epoch 51/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0069\n",
            "Epoch 52/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0068\n",
            "Epoch 53/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0068\n",
            "Epoch 54/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0067\n",
            "Epoch 55/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0069\n",
            "Epoch 56/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0068\n",
            "Epoch 57/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0067\n",
            "Epoch 58/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0067\n",
            "Epoch 59/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0069\n",
            "Epoch 60/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0067\n",
            "Epoch 61/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0069\n",
            "Epoch 62/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0113 - val_loss: 0.0067\n",
            "Epoch 63/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0065\n",
            "Epoch 64/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0064\n",
            "Epoch 65/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0068\n",
            "Epoch 66/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0067\n",
            "Epoch 67/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0080 - val_loss: 0.0059\n",
            "Epoch 68/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0079 - val_loss: 0.0060\n",
            "Epoch 69/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0073 - val_loss: 0.0055\n",
            "Epoch 70/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0059\n",
            "Epoch 71/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 0.0056\n",
            "Epoch 72/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0066 - val_loss: 0.0060\n",
            "Epoch 73/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0074 - val_loss: 0.0049\n",
            "Epoch 74/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0069 - val_loss: 0.0052\n",
            "Epoch 75/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0083 - val_loss: 0.0054\n",
            "Epoch 76/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0062\n",
            "Epoch 77/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0174 - val_loss: 0.0083\n",
            "Epoch 78/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0086\n",
            "Epoch 79/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0155 - val_loss: 0.0081\n",
            "Epoch 80/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0073\n",
            "Epoch 81/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0081\n",
            "Epoch 82/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0081\n",
            "Epoch 83/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0151 - val_loss: 0.0084\n",
            "Epoch 84/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0074\n",
            "Epoch 85/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0074\n",
            "Epoch 86/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0078\n",
            "Epoch 87/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0074\n",
            "Epoch 88/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0072\n",
            "Epoch 89/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0071\n",
            "Epoch 90/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0069\n",
            "Epoch 91/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0067\n",
            "Epoch 92/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0070\n",
            "Epoch 93/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0120 - val_loss: 0.0070\n",
            "Epoch 94/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0118 - val_loss: 0.0070\n",
            "Epoch 95/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0114 - val_loss: 0.0067\n",
            "Epoch 96/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0068\n",
            "Epoch 97/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0066\n",
            "Epoch 98/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0065\n",
            "Epoch 99/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0106 - val_loss: 0.0065\n",
            "Epoch 100/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 101/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0098 - val_loss: 0.0064\n",
            "Epoch 102/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0063\n",
            "Epoch 103/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0061\n",
            "Epoch 104/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0091 - val_loss: 0.0061\n",
            "Epoch 105/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0095 - val_loss: 0.0061\n",
            "Epoch 106/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0091 - val_loss: 0.0058\n",
            "Epoch 107/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0083 - val_loss: 0.0056\n",
            "Epoch 108/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0070 - val_loss: 0.0052\n",
            "Epoch 109/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0064 - val_loss: 0.0050\n",
            "Epoch 110/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0061 - val_loss: 0.0051\n",
            "Epoch 111/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0059 - val_loss: 0.0046\n",
            "Epoch 112/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0056 - val_loss: 0.0043\n",
            "Epoch 113/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0043\n",
            "Epoch 114/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0067 - val_loss: 0.0045\n",
            "Epoch 115/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0082 - val_loss: 0.0050\n",
            "Epoch 116/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0082\n",
            "Epoch 117/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0082\n",
            "Epoch 118/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0073\n",
            "Epoch 119/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0063\n",
            "Epoch 120/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0063\n",
            "Epoch 121/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0144 - val_loss: 0.0077\n",
            "Epoch 122/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0082\n",
            "Epoch 123/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0080\n",
            "Epoch 124/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0150 - val_loss: 0.0078\n",
            "Epoch 125/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0076\n",
            "Epoch 126/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0079\n",
            "Epoch 127/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0075\n",
            "Epoch 128/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0083\n",
            "Epoch 129/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 130/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 131/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0083\n",
            "Epoch 132/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0170 - val_loss: 0.0084\n",
            "Epoch 133/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0085\n",
            "Epoch 134/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0083\n",
            "Epoch 135/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 136/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0165 - val_loss: 0.0079\n",
            "Epoch 137/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0083\n",
            "Epoch 138/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0083\n",
            "Epoch 139/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0168 - val_loss: 0.0082\n",
            "Epoch 140/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0159 - val_loss: 0.0083\n",
            "Epoch 141/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0153 - val_loss: 0.0077\n",
            "Epoch 142/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0075\n",
            "Epoch 143/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0072\n",
            "Epoch 144/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0071\n",
            "Epoch 145/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0071\n",
            "Epoch 146/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0070\n",
            "Epoch 147/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0070\n",
            "Epoch 148/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0069\n",
            "Epoch 149/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0069\n",
            "Epoch 150/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0067\n",
            "Epoch 151/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0066\n",
            "Epoch 152/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0097 - val_loss: 0.0065\n",
            "Epoch 153/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 154/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0064\n",
            "Epoch 155/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 156/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0062\n",
            "Epoch 157/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0062\n",
            "Epoch 158/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0061\n",
            "Epoch 159/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0096 - val_loss: 0.0061\n",
            "Epoch 160/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0061\n",
            "Epoch 161/500\n",
            "78/78 [==============================] - 0s 4ms/step - loss: 0.0099 - val_loss: 0.0061\n",
            "Epoch 162/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0097 - val_loss: 0.0060\n",
            "Epoch 163/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0092 - val_loss: 0.0060\n",
            "Epoch 164/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0088 - val_loss: 0.0059\n",
            "Epoch 165/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0059\n",
            "Epoch 166/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0082 - val_loss: 0.0058\n",
            "Epoch 167/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0079 - val_loss: 0.0057\n",
            "Epoch 168/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0057\n",
            "Epoch 169/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0056\n",
            "Epoch 170/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0055\n",
            "Epoch 171/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0054\n",
            "Epoch 172/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0080 - val_loss: 0.0054\n",
            "Epoch 173/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0055\n",
            "Epoch 174/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0062\n",
            "Epoch 175/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0081\n",
            "Epoch 176/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0053\n",
            "Epoch 177/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0064 - val_loss: 0.0048\n",
            "Epoch 178/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0045\n",
            "Epoch 179/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0054 - val_loss: 0.0041\n",
            "Epoch 180/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0049 - val_loss: 0.0037\n",
            "Epoch 181/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0034\n",
            "Epoch 182/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0033\n",
            "Epoch 183/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 0.0033\n",
            "Epoch 184/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0031\n",
            "Epoch 185/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0028\n",
            "Epoch 186/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 0.0026\n",
            "Epoch 187/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 188/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 189/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 190/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 191/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 192/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.0029\n",
            "Epoch 193/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 194/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0023\n",
            "Epoch 195/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0020\n",
            "Epoch 196/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 197/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0019\n",
            "Epoch 198/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 199/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 200/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 0.0015\n",
            "Epoch 201/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 202/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 203/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0026 - val_loss: 0.0015\n",
            "Epoch 204/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 205/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0013\n",
            "Epoch 206/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0013\n",
            "Epoch 207/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.0013\n",
            "Epoch 208/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0026 - val_loss: 0.0012\n",
            "Epoch 209/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.0011\n",
            "Epoch 210/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0011\n",
            "Epoch 211/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.0011\n",
            "Epoch 212/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0011\n",
            "Epoch 213/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0012\n",
            "Epoch 214/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0059 - val_loss: 0.0014\n",
            "Epoch 215/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0066 - val_loss: 0.0013\n",
            "Epoch 216/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0027\n",
            "Epoch 217/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0040\n",
            "Epoch 218/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0075\n",
            "Epoch 219/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0127 - val_loss: 0.0054\n",
            "Epoch 220/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0087 - val_loss: 0.0044\n",
            "Epoch 221/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0061 - val_loss: 0.0026\n",
            "Epoch 222/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0046 - val_loss: 0.0021\n",
            "Epoch 223/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0086 - val_loss: 0.0018\n",
            "Epoch 224/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 0.0020\n",
            "Epoch 225/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0050\n",
            "Epoch 226/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0040\n",
            "Epoch 227/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0056 - val_loss: 0.0021\n",
            "Epoch 228/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0022\n",
            "Epoch 229/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0071\n",
            "Epoch 230/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0122 - val_loss: 0.0049\n",
            "Epoch 231/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0071 - val_loss: 0.0022\n",
            "Epoch 232/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0027\n",
            "Epoch 233/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.0018\n",
            "Epoch 234/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0018\n",
            "Epoch 235/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0047 - val_loss: 0.0019\n",
            "Epoch 236/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0019\n",
            "Epoch 237/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0022\n",
            "Epoch 238/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 0.0021\n",
            "Epoch 239/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0066 - val_loss: 0.0020\n",
            "Epoch 240/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0053 - val_loss: 0.0025\n",
            "Epoch 241/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0044\n",
            "Epoch 242/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0076 - val_loss: 0.0018\n",
            "Epoch 243/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 244/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0073 - val_loss: 0.0026\n",
            "Epoch 245/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0065 - val_loss: 0.0023\n",
            "Epoch 246/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0067 - val_loss: 0.0023\n",
            "Epoch 247/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0053 - val_loss: 0.0019\n",
            "Epoch 248/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0019\n",
            "Epoch 249/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0018\n",
            "Epoch 250/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0020\n",
            "Epoch 251/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 252/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0015\n",
            "Epoch 253/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0030 - val_loss: 0.0015\n",
            "Epoch 254/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 255/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 256/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 257/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0026 - val_loss: 0.0014\n",
            "Epoch 258/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0015\n",
            "Epoch 259/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0013\n",
            "Epoch 260/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.0013\n",
            "Epoch 261/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0022 - val_loss: 0.0011\n",
            "Epoch 262/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 9.8454e-04\n",
            "Epoch 263/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.0011\n",
            "Epoch 264/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.0010\n",
            "Epoch 265/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0021 - val_loss: 0.0012\n",
            "Epoch 266/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 267/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.0012\n",
            "Epoch 268/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 269/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 9.6193e-04\n",
            "Epoch 270/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0012\n",
            "Epoch 271/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 9.8635e-04\n",
            "Epoch 272/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0034 - val_loss: 0.0012\n",
            "Epoch 273/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 0.0011\n",
            "Epoch 274/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.0012\n",
            "Epoch 275/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0024 - val_loss: 0.0017\n",
            "Epoch 276/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0012\n",
            "Epoch 277/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 278/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0011\n",
            "Epoch 279/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0022 - val_loss: 0.0013\n",
            "Epoch 280/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.0010\n",
            "Epoch 281/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.0010\n",
            "Epoch 282/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0026 - val_loss: 0.0010\n",
            "Epoch 283/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0027 - val_loss: 0.0010\n",
            "Epoch 284/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0027 - val_loss: 0.0011\n",
            "Epoch 285/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0013\n",
            "Epoch 286/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0024 - val_loss: 0.0014\n",
            "Epoch 287/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 288/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0022 - val_loss: 0.0015\n",
            "Epoch 289/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0022 - val_loss: 0.0016\n",
            "Epoch 290/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 291/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 292/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 293/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 294/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0020 - val_loss: 0.0017\n",
            "Epoch 295/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 296/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 297/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 298/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 299/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 300/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 301/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 302/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 303/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 304/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.0011\n",
            "Epoch 305/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 306/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 307/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 308/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 309/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 310/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 311/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 312/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 313/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 314/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 315/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 316/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 317/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 318/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 319/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0023\n",
            "Epoch 320/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0015\n",
            "Epoch 321/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 322/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 8.4216e-04\n",
            "Epoch 323/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0075 - val_loss: 0.0011\n",
            "Epoch 324/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0070 - val_loss: 9.7202e-04\n",
            "Epoch 325/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0026\n",
            "Epoch 326/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0028\n",
            "Epoch 327/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0195 - val_loss: 0.0082\n",
            "Epoch 328/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0166 - val_loss: 0.0082\n",
            "Epoch 329/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0162 - val_loss: 0.0079\n",
            "Epoch 330/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.0080\n",
            "Epoch 331/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0066\n",
            "Epoch 332/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0025\n",
            "Epoch 333/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.0028\n",
            "Epoch 334/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0152 - val_loss: 0.0082\n",
            "Epoch 335/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0169 - val_loss: 0.0083\n",
            "Epoch 336/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0171 - val_loss: 0.0082\n",
            "Epoch 337/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0167 - val_loss: 0.0079\n",
            "Epoch 338/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0149 - val_loss: 0.0043\n",
            "Epoch 339/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0151 - val_loss: 0.0078\n",
            "Epoch 340/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0142 - val_loss: 0.0059\n",
            "Epoch 341/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0052\n",
            "Epoch 342/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0087 - val_loss: 0.0051\n",
            "Epoch 343/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0080 - val_loss: 0.0050\n",
            "Epoch 344/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0051\n",
            "Epoch 345/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0074 - val_loss: 0.0047\n",
            "Epoch 346/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0045\n",
            "Epoch 347/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0067 - val_loss: 0.0043\n",
            "Epoch 348/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.0036\n",
            "Epoch 349/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0022\n",
            "Epoch 350/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0020\n",
            "Epoch 351/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0082\n",
            "Epoch 352/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0166 - val_loss: 0.0075\n",
            "Epoch 353/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0154 - val_loss: 0.0074\n",
            "Epoch 354/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0031\n",
            "Epoch 355/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0084 - val_loss: 0.0025\n",
            "Epoch 356/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0168 - val_loss: 0.0042\n",
            "Epoch 357/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0171 - val_loss: 0.0081\n",
            "Epoch 358/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0161 - val_loss: 0.0080\n",
            "Epoch 359/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0172 - val_loss: 0.0066\n",
            "Epoch 360/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0013\n",
            "Epoch 361/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0073 - val_loss: 0.0017\n",
            "Epoch 362/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0052\n",
            "Epoch 363/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0095 - val_loss: 0.0043\n",
            "Epoch 364/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0077 - val_loss: 0.0040\n",
            "Epoch 365/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0071 - val_loss: 0.0032\n",
            "Epoch 366/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0015\n",
            "Epoch 367/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 368/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0148 - val_loss: 0.0069\n",
            "Epoch 369/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0036\n",
            "Epoch 370/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0012\n",
            "Epoch 371/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0046 - val_loss: 0.0014\n",
            "Epoch 372/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0062\n",
            "Epoch 373/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0044\n",
            "Epoch 374/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0013\n",
            "Epoch 375/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0015\n",
            "Epoch 376/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0035\n",
            "Epoch 377/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0040\n",
            "Epoch 378/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0061 - val_loss: 0.0072\n",
            "Epoch 379/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0034\n",
            "Epoch 380/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0069 - val_loss: 0.0020\n",
            "Epoch 381/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0012\n",
            "Epoch 382/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 0.0012\n",
            "Epoch 383/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0011\n",
            "Epoch 384/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 385/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 0.0012\n",
            "Epoch 386/500\n",
            "78/78 [==============================] - 1s 9ms/step - loss: 0.0029 - val_loss: 0.0011\n",
            "Epoch 387/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 0.0010\n",
            "Epoch 388/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0027 - val_loss: 0.0014\n",
            "Epoch 389/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0030 - val_loss: 9.5728e-04\n",
            "Epoch 390/500\n",
            "78/78 [==============================] - 1s 8ms/step - loss: 0.0026 - val_loss: 0.0012\n",
            "Epoch 391/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0030 - val_loss: 9.5378e-04\n",
            "Epoch 392/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0025 - val_loss: 0.0011\n",
            "Epoch 393/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0031 - val_loss: 9.3104e-04\n",
            "Epoch 394/500\n",
            "78/78 [==============================] - 1s 19ms/step - loss: 0.0026 - val_loss: 9.9455e-04\n",
            "Epoch 395/500\n",
            "78/78 [==============================] - 1s 18ms/step - loss: 0.0033 - val_loss: 9.2843e-04\n",
            "Epoch 396/500\n",
            "78/78 [==============================] - 2s 22ms/step - loss: 0.0028 - val_loss: 9.3953e-04\n",
            "Epoch 397/500\n",
            "78/78 [==============================] - 2s 22ms/step - loss: 0.0035 - val_loss: 9.6106e-04\n",
            "Epoch 398/500\n",
            "78/78 [==============================] - 1s 14ms/step - loss: 0.0033 - val_loss: 9.8187e-04\n",
            "Epoch 399/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0042 - val_loss: 0.0011\n",
            "Epoch 400/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 8.5037e-04\n",
            "Epoch 401/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0064 - val_loss: 9.9119e-04\n",
            "Epoch 402/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 8.4720e-04\n",
            "Epoch 403/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0070 - val_loss: 0.0010\n",
            "Epoch 404/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0064 - val_loss: 9.1508e-04\n",
            "Epoch 405/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0052\n",
            "Epoch 406/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0070 - val_loss: 0.0041\n",
            "Epoch 407/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0063 - val_loss: 0.0042\n",
            "Epoch 408/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0062 - val_loss: 0.0037\n",
            "Epoch 409/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0062 - val_loss: 0.0034\n",
            "Epoch 410/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0027\n",
            "Epoch 411/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0049 - val_loss: 0.0011\n",
            "Epoch 412/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 0.0011\n",
            "Epoch 413/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0073 - val_loss: 8.5499e-04\n",
            "Epoch 414/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0037\n",
            "Epoch 415/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0060 - val_loss: 0.0039\n",
            "Epoch 416/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0051 - val_loss: 0.0034\n",
            "Epoch 417/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0062 - val_loss: 0.0038\n",
            "Epoch 418/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0059 - val_loss: 0.0033\n",
            "Epoch 419/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0061 - val_loss: 0.0030\n",
            "Epoch 420/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0057 - val_loss: 0.0031\n",
            "Epoch 421/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 0.0029\n",
            "Epoch 422/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 0.0029\n",
            "Epoch 423/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0054 - val_loss: 0.0028\n",
            "Epoch 424/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0048 - val_loss: 0.0037\n",
            "Epoch 425/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 0.0027\n",
            "Epoch 426/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 0.0029\n",
            "Epoch 427/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0016\n",
            "Epoch 428/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 429/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0055 - val_loss: 7.6421e-04\n",
            "Epoch 430/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0086 - val_loss: 8.8124e-04\n",
            "Epoch 431/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0034\n",
            "Epoch 432/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0044 - val_loss: 0.0025\n",
            "Epoch 433/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0025\n",
            "Epoch 434/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0025\n",
            "Epoch 435/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0024\n",
            "Epoch 436/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0045 - val_loss: 0.0022\n",
            "Epoch 437/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 438/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.0013\n",
            "Epoch 439/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0078 - val_loss: 9.6094e-04\n",
            "Epoch 440/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0080\n",
            "Epoch 441/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0161 - val_loss: 0.0079\n",
            "Epoch 442/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0062\n",
            "Epoch 443/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0015\n",
            "Epoch 444/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0086 - val_loss: 0.0021\n",
            "Epoch 445/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0159 - val_loss: 0.0072\n",
            "Epoch 446/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0023\n",
            "Epoch 447/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0091 - val_loss: 0.0022\n",
            "Epoch 448/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0010\n",
            "Epoch 449/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0080\n",
            "Epoch 450/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0015\n",
            "Epoch 451/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0082 - val_loss: 0.0022\n",
            "Epoch 452/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0075 - val_loss: 0.0022\n",
            "Epoch 453/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0075\n",
            "Epoch 454/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0079\n",
            "Epoch 455/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0135 - val_loss: 0.0069\n",
            "Epoch 456/500\n",
            "78/78 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0016\n",
            "Epoch 457/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0076 - val_loss: 0.0011\n",
            "Epoch 458/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0058 - val_loss: 9.9232e-04\n",
            "Epoch 459/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0029\n",
            "Epoch 460/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0076\n",
            "Epoch 461/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0079\n",
            "Epoch 462/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0082\n",
            "Epoch 463/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0085\n",
            "Epoch 464/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0075\n",
            "Epoch 465/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0074\n",
            "Epoch 466/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0072\n",
            "Epoch 467/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0067\n",
            "Epoch 468/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0056\n",
            "Epoch 469/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0023\n",
            "Epoch 470/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0077 - val_loss: 0.0034\n",
            "Epoch 471/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0021\n",
            "Epoch 472/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0028\n",
            "Epoch 473/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0077\n",
            "Epoch 474/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0029\n",
            "Epoch 475/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0080\n",
            "Epoch 476/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0068\n",
            "Epoch 477/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0063\n",
            "Epoch 478/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0048\n",
            "Epoch 479/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0018\n",
            "Epoch 480/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0067 - val_loss: 0.0022\n",
            "Epoch 481/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0085 - val_loss: 0.0039\n",
            "Epoch 482/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0054 - val_loss: 0.0030\n",
            "Epoch 483/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0024\n",
            "Epoch 484/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0071 - val_loss: 0.0022\n",
            "Epoch 485/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0079 - val_loss: 0.0029\n",
            "Epoch 486/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0041 - val_loss: 0.0026\n",
            "Epoch 487/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0028 - val_loss: 0.0019\n",
            "Epoch 488/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 489/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 0.0026\n",
            "Epoch 490/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0031 - val_loss: 0.0020\n",
            "Epoch 491/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 492/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0047 - val_loss: 0.0016\n",
            "Epoch 493/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0043 - val_loss: 0.0015\n",
            "Epoch 494/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 495/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 496/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0044 - val_loss: 0.0014\n",
            "Epoch 497/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 498/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 499/500\n",
            "78/78 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 500/500\n",
            "78/78 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 0.0013\n"
          ]
        }
      ],
      "source": [
        "a = tf.Variable(0.45, name=\"a\", trainable=True, dtype=tf.float32)\n",
        "b = tf.Variable(0.5, name=\"b\", trainable=True, dtype=tf.float32)\n",
        "c = tf.Variable(-0.5, name=\"c\", trainable=True, dtype=tf.float32)\n",
        "def loss_fn(y_true, y_pred):\n",
        "    squared_difference = tf.square(y_true[:,0] - y_pred[:,0])\n",
        "    squared_difference2 = tf.square(y_true[:,2]-y_pred[:,2])\n",
        "    squared_difference1 = tf.square(y_true[:,1]-y_pred[:,1])    \n",
        "    squared_difference3 = tf.square(y_pred[:,2] + a*y_pred[:,0]*y_pred[:,1] +c*y_pred[:,0] +b* y_pred[:,0]**3)\n",
        "    return tf.reduce_mean(squared_difference, axis=-1) + 0.2*tf.reduce_mean(squared_difference3, axis=-1)\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "model.add(Dense(9))\n",
        "model.compile(loss=loss_fn, optimizer='adam')\n",
        "history = model.fit(trainX, trainY, epochs=500, batch_size=64, validation_data=(testX, testY), shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 1s 2ms/step\n",
            "(1240, 9)\n",
            "(1240, 1096)\n",
            "Test RMSE: 1.655\n",
            "Test MAE: 1.616\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "yhat = model.predict(testX)\n",
        "print(yhat.shape)\n",
        "testX = testX.reshape((testX.shape[0], testX.shape[2]))\n",
        "print(testX.shape)\n",
        "inv_yhat = np.concatenate((testX, yhat), axis=1)\n",
        "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "inv_yhat1 = inv_yhat[:, -3:]\n",
        "inv_yhat = inv_yhat[:, -3]\n",
        "inv_y = np.concatenate((testX, testY), axis=1)\n",
        "inv_y = scaler.inverse_transform(inv_y)\n",
        "inv_y1 = inv_y[:, -3:]\n",
        "inv_y = inv_y[:, -3]\n",
        "rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "mae = mean_absolute_error(inv_y, inv_yhat)\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
